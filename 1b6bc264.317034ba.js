(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{148:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return i})),a.d(t,"metadata",(function(){return o})),a.d(t,"rightToc",(function(){return p})),a.d(t,"default",(function(){return s}));var n=a(2),r=a(10),c=(a(0),a(181)),i={id:"m4c_captioner",sidebar_label:"M4C-Captioner",title:"TextCaps: a Dataset for Image Captioning with Reading Comprehension"},o={id:"projects/m4c_captioner",title:"TextCaps: a Dataset for Image Captioning with Reading Comprehension",description:"This project page shows how to use M4C-Captioner model from the following paper, released under the MMF:",source:"@site/docs/projects/m4c_captioner.md",permalink:"/docs/projects/m4c_captioner",editUrl:"https://github.com/facebookresearch/mmf/edit/master/website/docs/projects/m4c_captioner.md",lastUpdatedBy:"Ronghang Hu",lastUpdatedAt:1599755654,sidebar_label:"M4C-Captioner",sidebar:"docs",previous:{title:"Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA",permalink:"/docs/projects/m4c"},next:{title:"MoViE+MCAN (VQA 2020 Challenge Winner)",permalink:"/docs/projects/movie_mcan"}},p=[{value:"Installation",id:"installation",children:[]},{value:"Pretrained M4C-Captioner Models",id:"pretrained-m4c-captioner-models",children:[]},{value:"Training and Evaluating M4C-Captioner",id:"training-and-evaluating-m4c-captioner",children:[]}],l={rightToc:p};function s(e){var t=e.components,a=Object(r.a)(e,["components"]);return Object(c.b)("wrapper",Object(n.a)({},l,a,{components:t,mdxType:"MDXLayout"}),Object(c.b)("p",null,"This project page shows how to use M4C-Captioner model from the following paper, released under the MMF:"),Object(c.b)("ul",null,Object(c.b)("li",{parentName:"ul"},"O. Sidorov, R. Hu, M. Rohrbach, A. Singh, ",Object(c.b)("em",{parentName:"li"},"TextCaps: a Dataset for Image Captioning with Reading Comprehension"),". in ECCV, 2020 (",Object(c.b)("a",Object(n.a)({parentName:"li"},{href:"https://arxiv.org/pdf/2003.12462.pdf"}),"PDF"),")")),Object(c.b)("pre",null,Object(c.b)("code",Object(n.a)({parentName:"pre"},{}),"@inproceedings{sidorov2019textcaps,\n  title={TextCaps: a Dataset for Image Captioningwith Reading Comprehension},\n  author={Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet},\n  booktitle={European Conference on Computer Vision},\n  year={2020}\n}\n")),Object(c.b)("p",null,"Project Page: ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"https://textvqa.org/textcaps"}),"https://textvqa.org/textcaps")),Object(c.b)("h2",{id:"installation"},"Installation"),Object(c.b)("p",null,"Install MMF following the ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"https://mmf.sh/docs/getting_started/installation/"}),"installation guide"),"."),Object(c.b)("p",null,"This will install all M4C dependencies such as ",Object(c.b)("inlineCode",{parentName:"p"},"transformers")," and ",Object(c.b)("inlineCode",{parentName:"p"},"editdistance")," and will also compile the python interface for PHOC features."),Object(c.b)("p",null,"In addition, it is also necessary to install ",Object(c.b)("inlineCode",{parentName:"p"},"pycocoevalcap"),":"),Object(c.b)("pre",null,Object(c.b)("code",Object(n.a)({parentName:"pre"},{}),"# install pycocoevalcap\n# use the repo below instead of https://github.com/tylin/coco-caption\n# note: you also need to have java on your machine\npip install git+https://github.com/ronghanghu/coco-caption.git@python23\n")),Object(c.b)("p",null,Object(c.b)("strong",{parentName:"p"},"Note that java is required for ",Object(c.b)("a",Object(n.a)({parentName:"strong"},{href:"https://github.com/ronghanghu/coco-caption"}),Object(c.b)("inlineCode",{parentName:"a"},"pycocoevalcap"))),"."),Object(c.b)("h2",{id:"pretrained-m4c-captioner-models"},"Pretrained M4C-Captioner Models"),Object(c.b)("p",null,"We release two variants of the M4C-Captioner model trained on the TextCaps dataset, one trained with newer features extracted with maskrcnn-benchmark (",Object(c.b)("inlineCode",{parentName:"p"},"defaults"),"), and the other trained with older features extracted with Caffe2 (",Object(c.b)("inlineCode",{parentName:"p"},"with_caffe2_feat"),"), which is used in our experimentations in the paper and has higher CIDEr. ",Object(c.b)("strong",{parentName:"p"},"Please use ",Object(c.b)("inlineCode",{parentName:"strong"},"with_caffe2_feat")," config and model zoo file if you would like to exactly reproduce the results from our paper.")),Object(c.b)("table",null,Object(c.b)("thead",{parentName:"table"},Object(c.b)("tr",{parentName:"thead"},Object(c.b)("th",Object(n.a)({parentName:"tr"},{align:null}),"Config Files (under ",Object(c.b)("inlineCode",{parentName:"th"},"projects/m4c_captioner/configs/m4c_captioner/textcaps"),")"),Object(c.b)("th",Object(n.a)({parentName:"tr"},{align:null}),"Pretrained Model Key"),Object(c.b)("th",Object(n.a)({parentName:"tr"},{align:null}),"Metrics"),Object(c.b)("th",Object(n.a)({parentName:"tr"},{align:null}),"Notes"))),Object(c.b)("tbody",{parentName:"table"},Object(c.b)("tr",{parentName:"tbody"},Object(c.b)("td",Object(n.a)({parentName:"tr"},{align:null}),Object(c.b)("inlineCode",{parentName:"td"},"defaults.yaml")),Object(c.b)("td",Object(n.a)({parentName:"tr"},{align:null}),Object(c.b)("inlineCode",{parentName:"td"},"m4c_captioner.textcaps.defaults")),Object(c.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"val CIDEr -- 89.1 (BLEU-4 -- 23.4)"),Object(c.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"newer features extracted with maskrcnn-benchmark")),Object(c.b)("tr",{parentName:"tbody"},Object(c.b)("td",Object(n.a)({parentName:"tr"},{align:null}),Object(c.b)("inlineCode",{parentName:"td"},"with_caffe2_feat.yaml")),Object(c.b)("td",Object(n.a)({parentName:"tr"},{align:null}),Object(c.b)("inlineCode",{parentName:"td"},"m4c_captioner.textcaps.with_caffe2_feat")),Object(c.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"val CIDEr -- 89.6 (BLEU-4 -- 23.3)"),Object(c.b)("td",Object(n.a)({parentName:"tr"},{align:null}),"older features extracted with Caffe2; ",Object(c.b)("strong",{parentName:"td"},"used in experiments in the paper"))))),Object(c.b)("h2",{id:"training-and-evaluating-m4c-captioner"},"Training and Evaluating M4C-Captioner"),Object(c.b)("p",null,"Please follow the ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"https://learnpythia.readthedocs.io/en/latest/tutorials/quickstart.html#training"}),"MMF documentation")," for the training and evaluation of the M4C-Captioner models."),Object(c.b)("p",null,"For example:"),Object(c.b)("p",null,"1) to train the M4C-Captioner model on the TextCaps training set:"),Object(c.b)("pre",null,Object(c.b)("code",Object(n.a)({parentName:"pre"},{className:"language-bash"}),"mmf_run datasets=textcaps \\\n    model=m4c_captioner \\\n    config=projects/m4c_captioner/configs/m4c_captioner/textcaps/defaults.yaml \\\n    env.save_dir=./save/m4c_captioner/defaults \\\n    run_type=train_val    \n")),Object(c.b)("p",null,"(Replace ",Object(c.b)("inlineCode",{parentName:"p"},"projects/m4c_captioner/configs/m4c_captioner/textcaps/defaults.yaml")," with other config files to train with other configurations. See the table above. You can also specify a different path to ",Object(c.b)("inlineCode",{parentName:"p"},"env.save_dir")," to save to a location you prefer.)"),Object(c.b)("p",null,"2) to generate prediction json files for the TextCaps (assuming you are evaluating the pretrained model ",Object(c.b)("inlineCode",{parentName:"p"},"m4c_captioner.textcaps.defaults"),"):"),Object(c.b)("p",null,"Generate prediction file on the validation set:"),Object(c.b)("pre",null,Object(c.b)("code",Object(n.a)({parentName:"pre"},{className:"language-bash"}),"mmf_predict datasets=textcaps \\\n    model=m4c_captioner \\\n    config=projects/m4c_captioner/configs/m4c_captioner/textcaps/defaults.yaml \\\n    env.save_dir=./save/m4c_captioner/defaults \\\n    run_type=val \\\n    checkpoint.resume_zoo=m4c_captioner.textcaps.defaults\n")),Object(c.b)("p",null,"Generate prediction file on the test set:"),Object(c.b)("pre",null,Object(c.b)("code",Object(n.a)({parentName:"pre"},{className:"language-bash"}),"mmf_predict datasets=textcaps \\\n    model=m4c_captioner \\\n    config=projects/m4c_captioner/configs/m4c_captioner/textcaps/defaults.yaml \\\n    env.save_dir=./save/m4c_captioner/defaults \\\n    run_type=test \\\n    checkpoint.resume_zoo=m4c_captioner.textcaps.defaults\n")),Object(c.b)("p",null,"As with training, you can replace ",Object(c.b)("inlineCode",{parentName:"p"},"config")," and ",Object(c.b)("inlineCode",{parentName:"p"},"checkpoint.resume_zoo")," according to the setting you want to evaluate."),Object(c.b)("div",{className:"admonition admonition-note alert alert--secondary"},Object(c.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(c.b)("h5",{parentName:"div"},Object(c.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(c.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"14",height:"16",viewBox:"0 0 14 16"}),Object(c.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M6.3 5.69a.942.942 0 0 1-.28-.7c0-.28.09-.52.28-.7.19-.18.42-.28.7-.28.28 0 .52.09.7.28.18.19.28.42.28.7 0 .28-.09.52-.28.7a1 1 0 0 1-.7.3c-.28 0-.52-.11-.7-.3zM8 7.99c-.02-.25-.11-.48-.31-.69-.2-.19-.42-.3-.69-.31H6c-.27.02-.48.13-.69.31-.2.2-.3.44-.31.69h1v3c.02.27.11.5.31.69.2.2.42.31.69.31h1c.27 0 .48-.11.69-.31.2-.19.3-.42.31-.69H8V7.98v.01zM7 2.3c-3.14 0-5.7 2.54-5.7 5.68 0 3.14 2.56 5.7 5.7 5.7s5.7-2.55 5.7-5.7c0-3.15-2.56-5.69-5.7-5.69v.01zM7 .98c3.86 0 7 3.14 7 7s-3.14 7-7 7-7-3.12-7-7 3.14-7 7-7z"})))),"note")),Object(c.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(c.b)("p",{parentName:"div"},"Use ",Object(c.b)("inlineCode",{parentName:"p"},"checkpoint.resume=True")," AND ",Object(c.b)("inlineCode",{parentName:"p"},"checkpoint.resume_best=True")," instead of ",Object(c.b)("inlineCode",{parentName:"p"},"checkpoint.resume_zoo=m4c_captioner.textcaps.defaults")," to evaluate your trained snapshots."))),Object(c.b)("div",{className:"admonition admonition-tip alert alert--success"},Object(c.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-heading"}),Object(c.b)("h5",{parentName:"div"},Object(c.b)("span",Object(n.a)({parentName:"h5"},{className:"admonition-icon"}),Object(c.b)("svg",Object(n.a)({parentName:"span"},{xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"}),Object(c.b)("path",Object(n.a)({parentName:"svg"},{fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"})))),"tip")),Object(c.b)("div",Object(n.a)({parentName:"div"},{className:"admonition-content"}),Object(c.b)("p",{parentName:"div"},"Follow ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"https://mmf.sh/docs/tutorials/checkpointing"}),"checkpointing")," tutorial to understand more fine-grained details of checkpoint, loading and resuming in MMF"))),Object(c.b)("p",null,"Afterwards, use ",Object(c.b)("inlineCode",{parentName:"p"},"projects/m4c_captioner/scripts/textcaps_eval.py")," to evaluate the prediction json file. For example:"),Object(c.b)("pre",null,Object(c.b)("code",Object(n.a)({parentName:"pre"},{className:"language-bash"}),"# the default data location of MMF (unless you have specified it otherwise)\n# this is where MMF datasets are stored\nexport MMF_DATA_DIR=~/.cache/torch/mmf/data\n\npython projects/m4c_captioner/scripts/textcaps_eval.py \\\n    --set val \\\n    --annotation_file ${MMF_DATA_DIR}/datasets/textcaps/defaults/annotations/imdb_val.npy \\\n    --pred_file YOUR_VAL_PREDICTION_FILE\n")),Object(c.b)("p",null,"For test set evaluation, please submit to the TextCaps EvalAI server. See ",Object(c.b)("a",Object(n.a)({parentName:"p"},{href:"https://textvqa.org/textcaps"}),"https://textvqa.org/textcaps")," for details."))}s.isMDXComponent=!0},181:function(e,t,a){"use strict";a.d(t,"a",(function(){return b})),a.d(t,"b",(function(){return u}));var n=a(0),r=a.n(n);function c(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){c(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function p(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},c=Object.keys(e);for(n=0;n<c.length;n++)a=c[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var c=Object.getOwnPropertySymbols(e);for(n=0;n<c.length;n++)a=c[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=r.a.createContext({}),s=function(e){var t=r.a.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},b=function(e){var t=s(e.components);return r.a.createElement(l.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},m=r.a.forwardRef((function(e,t){var a=e.components,n=e.mdxType,c=e.originalType,i=e.parentName,l=p(e,["components","mdxType","originalType","parentName"]),b=s(a),m=n,u=b["".concat(i,".").concat(m)]||b[m]||d[m]||c;return a?r.a.createElement(u,o(o({ref:t},l),{},{components:a})):r.a.createElement(u,o({ref:t},l))}));function u(e,t){var a=arguments,n=t&&t.mdxType;if("string"==typeof e||n){var c=a.length,i=new Array(c);i[0]=m;var o={};for(var p in t)hasOwnProperty.call(t,p)&&(o[p]=t[p]);o.originalType=e,o.mdxType="string"==typeof e?e:n,i[1]=o;for(var l=2;l<c;l++)i[l]=a[l];return r.a.createElement.apply(null,i)}return r.a.createElement.apply(null,a)}m.displayName="MDXCreateElement"}}]);