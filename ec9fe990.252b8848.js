(window.webpackJsonp=window.webpackJsonp||[]).push([[29],{176:function(e,t,r){"use strict";r.r(t),r.d(t,"frontMatter",(function(){return i})),r.d(t,"metadata",(function(){return c})),r.d(t,"rightToc",(function(){return b})),r.d(t,"default",(function(){return l}));var a=r(2),n=r(10),o=(r(0),r(181)),i={id:"model_zoo",title:"Model Zoo",sidebar_label:"Model Zoo"},c={id:"notes/model_zoo",title:"Model Zoo",description:"Here is the current list of models currently implemented in MMF:",source:"@site/docs/notes/model_zoo.md",permalink:"/docs/notes/model_zoo",editUrl:"https://github.com/facebookresearch/mmf/edit/master/website/docs/notes/model_zoo.md",lastUpdatedBy:"Vedanuj Goswami",lastUpdatedAt:1594064536,sidebar_label:"Model Zoo",sidebar:"docs",previous:{title:"Dataset Zoo",permalink:"/docs/notes/dataset_zoo"},next:{title:"Pretrained Models",permalink:"/docs/notes/pretrained_models"}},b=[],s={rightToc:b};function l(e){var t=e.components,r=Object(n.a)(e,["components"]);return Object(o.b)("wrapper",Object(a.a)({},s,r,{components:t,mdxType:"MDXLayout"}),Object(o.b)("p",null,"Here is the current list of models currently implemented in MMF:"),Object(o.b)("ul",null,Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"M4C")," Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://arxiv.org/abs/1911.06258"}),"arXiv"),"] [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://github.com/facebookresearch/mmf/tree/master/projects/m4c"}),"project"),"]"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"ViLBERT")," ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://arxiv.org/abs/1908.02265"}),"arXiv"),"] [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://github.com/facebookresearch/mmf/tree/master/projects/vilbert"}),"project"),"]"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"VisualBert")," Visualbert: A simple and performant baseline for vision and language [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://arxiv.org/abs/1908.03557"}),"arXiv"),"] [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://arxiv.org/abs/1908.03557"}),"project"),"]"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"LoRRA")," Towards VQA Models That Can Read [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://arxiv.org/abs/1904.08920"}),"arXiv"),"] [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://github.com/facebookresearch/mmf/tree/master/projects/lorra"}),"project"),"]"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"M4C Captioner")," TextCaps: a Dataset for Image Captioning with Reading Comprehension [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://arxiv.org/abs/2003.12462"}),"arXiv"),"] [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://github.com/facebookresearch/mmf/tree/master/projects/m4c_captioner"}),"project"),"]"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"Pythia")," Pythia v0. 1: the winning entry to the vqa challenge 2018 [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://arxiv.org/abs/1807.09956"}),"arXiv"),"] [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://github.com/facebookresearch/mmf/tree/master/projects/pythia"}),"project"),"]"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"BUTD")," Bottom-up and top-down attention for image captioning and visual question answering [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://arxiv.org/abs/1707.07998"}),"arXiv"),"] [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://github.com/facebookresearch/mmf/tree/master/projects/butd"}),"project"),"]"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"MMBT")," Supervised Multimodal Bitransformers for Classifying Images and Text [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://arxiv.org/abs/1909.02950"}),"arXiv"),"] [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://github.com/facebookresearch/mmf/tree/master/projects/mmbt"}),"project"),"]"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"MoViE")," Revisiting Modulated Convolutions for Visual Counting and Beyond [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://arxiv.org/abs/2004.11883"}),"arXiv"),"] [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://github.com/facebookresearch/mmf/tree/master/projects/movie_mcan"}),"project"),"]"),Object(o.b)("li",{parentName:"ul"},Object(o.b)("strong",{parentName:"li"},"BAN")," Bilinear Attention Networks [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://arxiv.org/abs/1805.07932"}),"arXiv"),"] [",Object(o.b)("a",Object(a.a)({parentName:"li"},{href:"https://github.com/facebookresearch/mmf/tree/master/projects/ban"}),"project"),"]")),Object(o.b)("p",null,"In addition to the above MMF also has implementations of models ConcatBERT, ConcatBOW, LateFusion, Unimodal Text, Unimodal Image, VisDial, MMFBERT etc. We are adding many more new models which will be available soon."))}l.isMDXComponent=!0},181:function(e,t,r){"use strict";r.d(t,"a",(function(){return p})),r.d(t,"b",(function(){return f}));var a=r(0),n=r.n(a);function o(e,t,r){return t in e?Object.defineProperty(e,t,{value:r,enumerable:!0,configurable:!0,writable:!0}):e[t]=r,e}function i(e,t){var r=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),r.push.apply(r,a)}return r}function c(e){for(var t=1;t<arguments.length;t++){var r=null!=arguments[t]?arguments[t]:{};t%2?i(Object(r),!0).forEach((function(t){o(e,t,r[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(r)):i(Object(r)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(r,t))}))}return e}function b(e,t){if(null==e)return{};var r,a,n=function(e,t){if(null==e)return{};var r,a,n={},o=Object.keys(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||(n[r]=e[r]);return n}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(a=0;a<o.length;a++)r=o[a],t.indexOf(r)>=0||Object.prototype.propertyIsEnumerable.call(e,r)&&(n[r]=e[r])}return n}var s=n.a.createContext({}),l=function(e){var t=n.a.useContext(s),r=t;return e&&(r="function"==typeof e?e(t):c(c({},t),e)),r},p=function(e){var t=l(e.components);return n.a.createElement(s.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return n.a.createElement(n.a.Fragment,{},t)}},u=n.a.forwardRef((function(e,t){var r=e.components,a=e.mdxType,o=e.originalType,i=e.parentName,s=b(e,["components","mdxType","originalType","parentName"]),p=l(r),u=a,f=p["".concat(i,".").concat(u)]||p[u]||m[u]||o;return r?n.a.createElement(f,c(c({ref:t},s),{},{components:r})):n.a.createElement(f,c({ref:t},s))}));function f(e,t){var r=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=r.length,i=new Array(o);i[0]=u;var c={};for(var b in t)hasOwnProperty.call(t,b)&&(c[b]=t[b]);c.originalType=e,c.mdxType="string"==typeof e?e:a,i[1]=c;for(var s=2;s<o;s++)i[s]=r[s];return n.a.createElement.apply(null,i)}return n.a.createElement.apply(null,r)}u.displayName="MDXCreateElement"}}]);